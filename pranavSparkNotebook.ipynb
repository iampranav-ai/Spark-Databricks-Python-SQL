{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pranav Spark Notebook (with comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"E-Commerce Analysis\").getOrCreate()\n",
    "\n",
    "# Read the CSV file\n",
    "df = spark.read.csv(\"e_commerce.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 1. Basic: Display schema\n",
    "df.printSchema()\n",
    "\n",
    "# 2. Basic: Show first 10 rows\n",
    "df.show(10)\n",
    "\n",
    "# 3. Basic: Count total number of rows\n",
    "total_rows = df.count()\n",
    "print(f\"Total number of rows: {total_rows}\")\n",
    "\n",
    "# 4. Medium: Get basic statistics of numeric columns\n",
    "df.select(\"quantity\", \"price\").describe().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Medium: Count distinct customers\n",
    "distinct_customers = df.select(\"customer_id\").distinct().count()\n",
    "print(f\"Number of distinct customers: {distinct_customers}\")\n",
    "\n",
    "# 6. Medium: Top 5 selling products\n",
    "df.groupBy(\"product_name\").agg(sum(\"quantity\").alias(\"total_quantity\")) \\\n",
    "  .orderBy(desc(\"total_quantity\")).show(5)\n",
    "\n",
    "# 7. Medium: Total revenue by category\n",
    "df.groupBy(\"category\").agg(sum(col(\"quantity\") * col(\"price\")).alias(\"total_revenue\")) \\\n",
    "  .orderBy(desc(\"total_revenue\")).show()\n",
    "\n",
    "# 8. Advanced: Daily revenue trend\n",
    "df.withColumn(\"date\", to_date(\"transaction_date\")) \\\n",
    "  .groupBy(\"date\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"daily_revenue\")) \\\n",
    "  .orderBy(\"date\").show()\n",
    "\n",
    "# 9. Advanced: Customer segmentation by total spend\n",
    "window_spec = Window.orderBy(desc(\"total_spend\"))\n",
    "customer_segments = df.groupBy(\"customer_id\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"total_spend\")) \\\n",
    "  .withColumn(\"rank\", dense_rank().over(window_spec)) \\\n",
    "  .withColumn(\"segment\", when(col(\"rank\") <= 100, \"Top 100\") \\\n",
    "                        .when(col(\"rank\") <= 1000, \"Top 1000\") \\\n",
    "                        .otherwise(\"Regular\"))\n",
    "customer_segments.show()\n",
    "\n",
    "# 10. Advanced: Product affinity analysis (products often bought together)\n",
    "from itertools import combinations\n",
    "product_pairs = df.groupBy(\"transaction_id\") \\\n",
    "  .agg(collect_set(\"product_name\").alias(\"products\"))\n",
    "product_pairs = product_pairs.withColumn(\"product_pairs\", explode(arrays_zip(\n",
    "    array([lit(x) for x in combinations(range(5), 2)]),\n",
    "    arrays_zip(*[slice(col(\"products\"), i, i+1) for i in range(5)])\n",
    ")))\n",
    "product_pairs.select(\"product_pairs.*\") \\\n",
    "  .groupBy(\"0\", \"1\") \\\n",
    "  .count() \\\n",
    "  .orderBy(desc(\"count\")) \\\n",
    "  .show(10)\n",
    "\n",
    "# 11. Basic: Most popular payment method\n",
    "df.groupBy(\"payment_method\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "# 12. Medium: Average order value\n",
    "avg_order_value = df.groupBy(\"transaction_id\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"order_value\")) \\\n",
    "  .select(avg(\"order_value\")).first()[0]\n",
    "print(f\"Average Order Value: ${avg_order_value:.2f}\")\n",
    "\n",
    "# 13. Medium: Monthly sales trend\n",
    "df.withColumn(\"year_month\", date_format(\"transaction_date\", \"yyyy-MM\")) \\\n",
    "  .groupBy(\"year_month\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"monthly_sales\")) \\\n",
    "  .orderBy(\"year_month\").show()\n",
    "\n",
    "# 14. Advanced: Customer lifetime value (CLV)\n",
    "clv = df.groupBy(\"customer_id\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"total_spend\"), \n",
    "       datediff(max(\"transaction_date\"), min(\"transaction_date\")).alias(\"days_as_customer\")) \\\n",
    "  .withColumn(\"clv\", col(\"total_spend\") / (col(\"days_as_customer\") / 365)) \\\n",
    "  .orderBy(desc(\"clv\"))\n",
    "clv.show()\n",
    "\n",
    "# 15. Advanced: Seasonal product analysis\n",
    "df.withColumn(\"month\", month(\"transaction_date\")) \\\n",
    "  .groupBy(\"month\", \"category\") \\\n",
    "  .agg(sum(\"quantity\").alias(\"total_quantity\")) \\\n",
    "  .orderBy(\"month\", desc(\"total_quantity\")) \\\n",
    "  .show()\n",
    "\n",
    "# 16. Basic: Number of partitions\n",
    "num_partitions = df.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "# 17. Advanced: Repartition for better performance\n",
    "df_repartitioned = df.repartition(100)  # Adjust the number based on your cluster size\n",
    "\n",
    "# 18. Advanced: Cache the dataframe for repeated use\n",
    "df_repartitioned.cache()\n",
    "\n",
    "# 19. Medium: Use Spark SQL for complex queries\n",
    "df.createOrReplaceTempView(\"e_commerce\")\n",
    "spark.sql(\"\"\"\n",
    "  SELECT category, subcategory, SUM(quantity * price) as revenue\n",
    "  FROM e_commerce\n",
    "  GROUP BY category, subcategory\n",
    "  ORDER BY revenue DESC\n",
    "  LIMIT 10\n",
    "\"\"\").show()\n",
    "\n",
    "# 20. Advanced: Window functions for customer ranking\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(desc(\"total_spend\"))\n",
    "customer_category_rank = df.groupBy(\"customer_id\", \"category\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"total_spend\")) \\\n",
    "  .withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "customer_category_rank.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Advanced: UDF for categorizing transaction time\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize_time(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    if 5 <= hour < 12:\n",
    "        return \"Morning\"\n",
    "    elif 12 <= hour < 17:\n",
    "        return \"Afternoon\"\n",
    "    elif 17 <= hour < 21:\n",
    "        return \"Evening\"\n",
    "    else:\n",
    "        return \"Night\"\n",
    "\n",
    "df_with_time_category = df.withColumn(\"time_category\", categorize_time(col(\"transaction_date\")))\n",
    "df_with_time_category.groupBy(\"time_category\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "# 22. Medium: Pivot table for category sales by month\n",
    "pivoted_sales = df.withColumn(\"month\", date_format(\"transaction_date\", \"MMM\")) \\\n",
    "  .groupBy(\"category\") \\\n",
    "  .pivot(\"month\") \\\n",
    "  .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"sales\")) \\\n",
    "  .orderBy(\"category\")\n",
    "pivoted_sales.show()\n",
    "\n",
    "# 23. Advanced: Analyze data skew\n",
    "df.groupBy(\"category\").count().orderBy(desc(\"count\")).show()\n",
    "\n",
    "# 24. Check executor and cluster details\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "# 25. Advanced: Use broadcast join for small lookup tables\n",
    "# Assuming we have a small product_info dataframe\n",
    "product_info = spark.createDataFrame([\n",
    "    (\"product_id1\", \"Brand A\"),\n",
    "    (\"product_id2\", \"Brand B\")\n",
    "], [\"product_id\", \"brand\"])\n",
    "\n",
    "broadcast_df = broadcast(product_info)\n",
    "df_with_brand = df.join(broadcast_df, \"product_id\", \"left\")\n",
    "\n",
    "# 26. Advanced: Write optimized parquet files\n",
    "df.write.partitionBy(\"category\").format(\"parquet\").mode(\"overwrite\").save(\"e_commerce_partitioned\")\n",
    "\n",
    "# 27. Advanced: Read optimized parquet files\n",
    "df_optimized = spark.read.parquet(\"e_commerce_partitioned\")\n",
    "\n",
    "# Remember to stop the Spark session when done\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Advanced: Cohort analysis for customer retention\n",
    "from pyspark.sql.functions import date_trunc, datediff, count, countDistinct\n",
    "\n",
    "df = df.withColumn(\"cohort_month\", date_trunc(\"month\", col(\"transaction_date\")))\n",
    "df = df.withColumn(\"months_since_first_purchase\", \n",
    "                   datediff(date_trunc(\"month\", col(\"transaction_date\")), \n",
    "                            col(\"cohort_month\")) / 30)\n",
    "\n",
    "cohort_analysis = df.groupBy(\"cohort_month\", \"months_since_first_purchase\") \\\n",
    "    .agg(countDistinct(\"customer_id\").alias(\"customer_count\")) \\\n",
    "    .orderBy(\"cohort_month\", \"months_since_first_purchase\")\n",
    "\n",
    "cohort_analysis.show()\n",
    "\n",
    "# 29. Advanced: RFM (Recency, Frequency, Monetary) Analysis\n",
    "from pyspark.sql.functions import max, datediff, count, sum\n",
    "\n",
    "current_date = df.agg(max(\"transaction_date\")).collect()[0][0]\n",
    "\n",
    "rfm = df.groupBy(\"customer_id\").agg(\n",
    "    datediff(lit(current_date), max(\"transaction_date\")).alias(\"recency\"),\n",
    "    count(\"transaction_id\").alias(\"frequency\"),\n",
    "    sum(col(\"quantity\") * col(\"price\")).alias(\"monetary\")\n",
    ")\n",
    "\n",
    "rfm.show()\n",
    "\n",
    "# 30. Medium: Calculate average time between purchases for each customer\n",
    "from pyspark.sql.functions import lag, avg\n",
    "\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "time_between_purchases = df.withColumn(\"prev_purchase_date\", \n",
    "                                       lag(\"transaction_date\").over(window_spec)) \\\n",
    "    .withColumn(\"days_between_purchases\", \n",
    "                datediff(col(\"transaction_date\"), col(\"prev_purchase_date\"))) \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(avg(\"days_between_purchases\").alias(\"avg_days_between_purchases\"))\n",
    "\n",
    "time_between_purchases.show()\n",
    "\n",
    "# 31. Advanced: Identify potential fraudulent transactions\n",
    "from pyspark.sql.functions import stddev\n",
    "\n",
    "avg_order_value = df.groupBy(\"customer_id\") \\\n",
    "    .agg(avg(col(\"quantity\") * col(\"price\")).alias(\"avg_order_value\"))\n",
    "\n",
    "stddev_order_value = df.groupBy(\"customer_id\") \\\n",
    "    .agg(stddev(col(\"quantity\") * col(\"price\")).alias(\"stddev_order_value\"))\n",
    "\n",
    "potential_fraud = df.join(avg_order_value, \"customer_id\") \\\n",
    "    .join(stddev_order_value, \"customer_id\") \\\n",
    "    .filter(col(\"quantity\") * col(\"price\") > col(\"avg_order_value\") + 3 * col(\"stddev_order_value\"))\n",
    "\n",
    "potential_fraud.show()\n",
    "\n",
    "# 32. Medium: Calculate running total of sales for each product\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(\"transaction_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "running_total = df.withColumn(\"running_total\", \n",
    "                              sum(col(\"quantity\") * col(\"price\")).over(window_spec))\n",
    "\n",
    "running_total.show()\n",
    "\n",
    "# 33. Advanced: Implement a recommendation system using ALS\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Prepare data for ALS\n",
    "als_data = df.select(\"customer_id\", \"product_id\", \"quantity\")\n",
    "\n",
    "# Split data into training and test sets\n",
    "(training, test) = als_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"customer_id\", itemCol=\"product_id\", ratingCol=\"quantity\",\n",
    "          coldStartStrategy=\"drop\")\n",
    "model = als.fit(training)\n",
    "\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"quantity\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error = {rmse}\")\n",
    "\n",
    "# Generate top 10 product recommendations for each user\n",
    "userRecs = model.recommendForAllUsers(10)\n",
    "userRecs.show()\n",
    "\n",
    "# 34. Medium: Analyze cart abandonment rate\n",
    "cart_abandonment = df.groupBy(\"customer_id\", \"transaction_id\") \\\n",
    "    .agg(sum(when(col(\"payment_method\").isNull(), 1).otherwise(0)).alias(\"abandoned_items\"),\n",
    "         count(\"*\").alias(\"total_items\")) \\\n",
    "    .withColumn(\"abandonment_rate\", col(\"abandoned_items\") / col(\"total_items\"))\n",
    "\n",
    "cart_abandonment.show()\n",
    "\n",
    "# 35. Advanced: Implement a custom Accumulator\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class SetAccumulator(AccumulatorParam):\n",
    "    def zero(self, initialValue):\n",
    "        return set(initialValue)\n",
    "\n",
    "    def addInPlace(self, v1, v2):\n",
    "        v1.update(v2)\n",
    "        return v1\n",
    "\n",
    "unique_products = spark.sparkContext.accumulator(set(), SetAccumulator())\n",
    "\n",
    "def add_products(row):\n",
    "    unique_products.add({row.product_id})\n",
    "\n",
    "df.foreach(add_products)\n",
    "\n",
    "print(f\"Number of unique products: {len(unique_products.value)}\")\n",
    "\n",
    "# 36. Advanced: Use Spark Streaming to process real-time sales data\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(spark.sparkContext, 1)  # 1-second batch interval\n",
    "\n",
    "# Assume we have a stream of sales data coming in\n",
    "sales_stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "\n",
    "# Process the stream\n",
    "parsed_sales = sales_stream.map(lambda line: line.split(\",\"))\n",
    "revenue_stream = parsed_sales.map(lambda x: (x[0], float(x[1]))) \\\n",
    "    .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "revenue_stream.pprint()\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "\n",
    "# 37. Medium: Use Spark MLlib for customer churn prediction\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Prepare features\n",
    "feature_columns = [\"total_spend\", \"frequency\", \"avg_order_value\", \"days_since_last_purchase\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df)\n",
    "\n",
    "# Split data into training and test sets\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model\n",
    "rf = RandomForestClassifier(labelCol=\"churn\", featuresCol=\"features\", numTrees=10)\n",
    "model = rf.fit(trainingData)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"churn\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "\n",
    "# 38. Advanced: Implement custom Partitioner for optimized joins\n",
    "from pyspark import RDD, Partitioner\n",
    "\n",
    "class CustomPartitioner(Partitioner):\n",
    "    def __init__(self, partitions):\n",
    "        self.partitions = partitions\n",
    "\n",
    "    def getPartition(self, key):\n",
    "        return hash(key) % self.partitions\n",
    "\n",
    "    def numPartitions(self):\n",
    "        return self.partitions\n",
    "\n",
    "# Use custom partitioner\n",
    "rdd = df.rdd.map(lambda row: (row.customer_id, row))\n",
    "partitioned_rdd = rdd.partitionBy(100, CustomPartitioner(100))\n",
    "\n",
    "# 39. Medium: Analyze customer lifetime value (CLV) distribution\n",
    "from pyspark.sql.functions import percentile_approx\n",
    "\n",
    "clv_distribution = df.groupBy(\"customer_id\") \\\n",
    "    .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"clv\")) \\\n",
    "    .select(\n",
    "        percentile_approx(\"clv\", array([0.25, 0.5, 0.75, 0.9, 0.95, 0.99]), 10000).alias(\"percentiles\")\n",
    "    ).first()\n",
    "\n",
    "print(\"CLV Distribution:\")\n",
    "print(f\"25th percentile: {clv_distribution.percentiles[0]}\")\n",
    "print(f\"Median: {clv_distribution.percentiles[1]}\")\n",
    "print(f\"75th percentile: {clv_distribution.percentiles[2]}\")\n",
    "print(f\"90th percentile: {clv_distribution.percentiles[3]}\")\n",
    "print(f\"95th percentile: {clv_distribution.percentiles[4]}\")\n",
    "print(f\"99th percentile: {clv_distribution.percentiles[5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40. Advanced: Implement custom UDF with Pandas UDF for better performance\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def calculate_discount(quantity: pd.Series, price: pd.Series) -> pd.Series:\n",
    "    return np.where(quantity * price > 1000, 0.1, 0.05)\n",
    "\n",
    "df_with_discount = df.withColumn(\"discount\", calculate_discount(col(\"quantity\"), col(\"price\")))\n",
    "df_with_discount.show()\n",
    "\n",
    "# 41. Medium: Analyze product return rate\n",
    "returns = spark.read.csv(\"returns.csv\", header=True, inferSchema=True)\n",
    "return_rate = df.join(returns, \"transaction_id\", \"left\") \\\n",
    "    .groupBy(\"product_id\") \\\n",
    "    .agg(\n",
    "        count(when(col(\"return_reason\").isNotNull(), True)).alias(\"returns\"),\n",
    "        count(\"*\").alias(\"total_sales\")\n",
    "    ) \\\n",
    "    .withColumn(\"return_rate\", col(\"returns\") / col(\"total_sales\"))\n",
    "\n",
    "return_rate.orderBy(desc(\"return_rate\")).show()\n",
    "\n",
    "# 42. Advanced: Implement A/B testing analysis\n",
    "ab_test = spark.read.csv(\"ab_test.csv\", header=True, inferSchema=True)\n",
    "ab_results = ab_test.groupBy(\"test_group\") \\\n",
    "    .agg(\n",
    "        avg(\"conversion_rate\").alias(\"avg_conversion_rate\"),\n",
    "        stddev(\"conversion_rate\").alias(\"stddev_conversion_rate\"),\n",
    "        count(\"*\").alias(\"sample_size\")\n",
    "    )\n",
    "\n",
    "ab_results.show()\n",
    "\n",
    "# 43. Medium: Analyze customer acquisition cost\n",
    "marketing_spend = spark.read.csv(\"marketing_spend.csv\", header=True, inferSchema=True)\n",
    "customer_acquisition = df.select(\"customer_id\").distinct() \\\n",
    "    .join(marketing_spend, \"date\") \\\n",
    "    .groupBy(\"channel\") \\\n",
    "    .agg(\n",
    "        count(\"customer_id\").alias(\"new_customers\"),\n",
    "        sum(\"spend\").alias(\"total_spend\")\n",
    "    ) \\\n",
    "    .withColumn(\"acquisition_cost\", col(\"total_spend\") / col(\"new_customers\"))\n",
    "\n",
    "customer_acquisition.show()\n",
    "\n",
    "# 44. Advanced: Implement custom Optimizer for Spark SQL\n",
    "from pyspark.sql.optimizer import Optimizer\n",
    "\n",
    "class CustomOptimizer(Optimizer):\n",
    "    def apply(self, plan):\n",
    "        # Implement custom optimization logic here\n",
    "        return plan\n",
    "\n",
    "spark.experimental.extraOptimizations = [CustomOptimizer()]\n",
    "\n",
    "# 45. Medium: Analyze product affinity using Market Basket Analysis\n",
    "from pyspark.ml.fpm import FPGrowth\n",
    "\n",
    "transactions = df.groupBy(\"transaction_id\") \\\n",
    "    .agg(collect_set(\"product_id\").alias(\"items\"))\n",
    "\n",
    "fp_growth = FPGrowth(itemsCol=\"items\", minSupport=0.01, minConfidence=0.5)\n",
    "model = fp_growth.fit(transactions)\n",
    "\n",
    "model.freqItemsets.show()\n",
    "model.associationRules.show()\n",
    "\n",
    "# 46. Advanced: Implement custom Broadcast variable for lookup tables\n",
    "product_lookup = spark.sparkContext.broadcast({\n",
    "    row[\"product_id\"]: row[\"product_name\"] \n",
    "    for row in df.select(\"product_id\", \"product_name\").distinct().collect()\n",
    "})\n",
    "\n",
    "def lookup_product_name(product_id):\n",
    "    return product_lookup.value.get(product_id, \"Unknown\")\n",
    "\n",
    "lookup_udf = udf(lookup_product_name)\n",
    "df_with_names = df.withColumn(\"product_name\", lookup_udf(col(\"product_id\")))\n",
    "df_with_names.show()\n",
    "\n",
    "# 47. Medium: Analyze seasonal trends in product categories\n",
    "seasonal_trends = df.withColumn(\"month\", month(\"transaction_date\")) \\\n",
    "    .groupBy(\"category\", \"month\") \\\n",
    "    .agg(sum(col(\"quantity\") * col(\"price\")).alias(\"revenue\")) \\\n",
    "    .orderBy(\"category\", \"month\")\n",
    "\n",
    "seasonal_trends.show()\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
